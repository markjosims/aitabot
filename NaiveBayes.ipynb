{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1031,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1032,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[AITA] I wrote an explanation in TIL and came ...</td>\n",
       "      <td>[Here is the post in question](http://www.redd...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[AITA] Threw my parent's donuts away</td>\n",
       "      <td>My parents are diabetic, morbidly obese, and a...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I told a goth girl she looked like a clown.</td>\n",
       "      <td>I was four.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AITA Had a disagreement about Les Miserables w...</td>\n",
       "      <td>I love the musical *Les Miserables*. A coworke...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[AITA] I 'hacked' our house router, changed th...</td>\n",
       "      <td>Backstory: I'm a semi-professional competitive...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  [AITA] I wrote an explanation in TIL and came ...   \n",
       "1               [AITA] Threw my parent's donuts away   \n",
       "2        I told a goth girl she looked like a clown.   \n",
       "3  AITA Had a disagreement about Les Miserables w...   \n",
       "4  [AITA] I 'hacked' our house router, changed th...   \n",
       "\n",
       "                                                body  class  \n",
       "0  [Here is the post in question](http://www.redd...   True  \n",
       "1  My parents are diabetic, morbidly obese, and a...   True  \n",
       "2                                        I was four.  False  \n",
       "3  I love the musical *Les Miserables*. A coworke...   True  \n",
       "4  Backstory: I'm a semi-professional competitive...   True  "
      ]
     },
     "execution_count": 1032,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "data = pd.read_csv('reddit_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the naive bayes classifier, we will aggregate text from title and body of submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1033,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>[AITA] I wrote an explanation in TIL and came ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>[AITA] Threw my parent's donuts away My parent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>I told a goth girl she looked like a clown. I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>AITA Had a disagreement about Les Miserables w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>[AITA] I 'hacked' our house router, changed th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                               text\n",
       "0   True  [AITA] I wrote an explanation in TIL and came ...\n",
       "1   True  [AITA] Threw my parent's donuts away My parent...\n",
       "2  False  I told a goth girl she looked like a clown. I ...\n",
       "3   True  AITA Had a disagreement about Les Miserables w...\n",
       "4   True  [AITA] I 'hacked' our house router, changed th..."
      ]
     },
     "execution_count": 1033,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [title + ' ' + body for title, body in zip( data['title'], data['body'] )]\n",
    "data['text'] = text\n",
    "data = data.drop(['title', 'body'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to drop the shortest records from the dataset. Since less than 10% of all records are less than 500 characters long, we'll drop those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1034,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2431, 2)\n",
      "2007.0255039078568\n",
      "2098.5321180555557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2304, 2)"
      ]
     },
     "execution_count": 1034,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.shape)\n",
    "data['text_len'] = [len(text) for text in data['text']]\n",
    "print(data['text_len'].mean())\n",
    "data = data[data['text_len'] > 500]\n",
    "print(data['text_len'].mean())\n",
    "data = data.drop(['text_len'], axis=1)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Using NLTK modules, we can remove stopwords (very common words that contribute little semantic content), remove punctuation, remove grammatical endings, and turn every string into a vector of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>[wrote, explanation, til, came, condescending,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>[threw, parent, donut, away, parent, diabetic,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>[disagreement, le, miserables, coworker, love,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>[hacked, house, router, changed, password, tur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>True</td>\n",
       "      <td>[atia, permanently, give, customer, decaf, nam...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                               text\n",
       "0   True  [wrote, explanation, til, came, condescending,...\n",
       "1   True  [threw, parent, donut, away, parent, diabetic,...\n",
       "3   True  [disagreement, le, miserables, coworker, love,...\n",
       "4   True  [hacked, house, router, changed, password, tur...\n",
       "5   True  [atia, permanently, give, customer, decaf, nam..."
      ]
     },
     "execution_count": 1035,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('aita') # common tag found in title\n",
    "lemmatizer = WordNetLemmatizer() # turns words into their base form, e.g. ran -> run\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\") # remove punctuation and tokenize\n",
    "\n",
    "def preprocess(string):\n",
    "    str_lower = string.lower()\n",
    "    str_no_underscore = str_lower.replace('_', ' ')\n",
    "    # remove numbers\n",
    "    str_no_num = re.sub(r'\\d+', '', str_no_underscore)\n",
    "    # tokenize and remove stopwords\n",
    "    tokens = tokenizer.tokenize(str_no_num)\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    # stem words\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    return tokens\n",
    "\n",
    "data['text'] = [preprocess(text) for text in data['text']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Hyper-Uncommon Words\n",
    "\n",
    "For our model to function best, we should remove any words that only appear in a few records. Most words appear in less than 10 documents, and so are considered noise. We'll also make a \"sparse\" dataset that only includes words that appear in at least 100 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1036,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16741\n",
      "uncommon: 13121\n",
      "sparse_uncommon: 13121\n"
     ]
    }
   ],
   "source": [
    "all_word_counts = defaultdict(list)\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    text = row['text']\n",
    "    for word in text:\n",
    "        all_word_counts[word].append(index)\n",
    "\n",
    "uncommon = {word: count for word, count in all_word_counts.items() if len(count) <= 10}\n",
    "sparse_uncommon = {word: count for word, count in all_word_counts.items() if len(count) <= 100}\n",
    "print(len(all_word_counts))\n",
    "print('uncommon:', len(uncommon))\n",
    "print('sparse_uncommon:', len(uncommon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go back to the records where these words appeared and remove them. Fortunately, it looks like no records were rendered empty by this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1037,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data before: (2304, 2)\n",
      "data after: (2304, 2)\n",
      "sparse data before: (2304, 2)\n",
      "sparse data after: (2304, 2)\n"
     ]
    }
   ],
   "source": [
    "sparse_data = data.copy()\n",
    "# pandas doesn't copy recursively\n",
    "sparse_data['text'] = [text.copy() for text in data['text']]\n",
    "\n",
    "for word, indices in uncommon.items():\n",
    "    for index in indices:\n",
    "        data.at[index, 'text'].remove(word)\n",
    "print('data before:', data.shape)\n",
    "data = data[[bool(x) for x in data['text']]]\n",
    "print('data after:', data.shape)\n",
    "\n",
    "\n",
    "for word, indices in sparse_uncommon.items():\n",
    "    for index in indices:\n",
    "        sparse_data.at[index, 'text'].remove(word)\n",
    "print('sparse data before:',sparse_data.shape)\n",
    "data = data[[bool(x) for x in data['text']]]\n",
    "print('sparse data after:',sparse_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing Word Counts\n",
    "In order to perform Naive Bayes, we need to turn each record's text into a vector of word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1038,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2304, 3603)"
      ]
     },
     "execution_count": 1038,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turn each list of tokens into a string to match input CountVectorizer expects\n",
    "data['text'] = [' '.join(sentence) for sentence in data['text']]\n",
    "sparse_data['text'] = [' '.join(sentence) for sentence in sparse_data['text']]\n",
    "\n",
    "count_vec = CountVectorizer()\n",
    "text_counts = count_vec.fit_transform(data['text'])\n",
    "sparse_text_counts = count_vec.fit_transform(sparse_data['text'])\n",
    "text_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then can turn the raw counts into counts of Term Frequency time inverse document frequency.\n",
    "This divides each word count by the total words in the document, and adjusts weights for words\n",
    "that occur in all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1039,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2304, 3603)"
      ]
     },
     "execution_count": 1039,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "transformed_counts = tfidf_transformer.fit_transform(text_counts)\n",
    "sparse_transformed_counts = tfidf_transformer.fit_transform(sparse_text_counts)\n",
    "transformed_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing with SMOTE\n",
    "\n",
    "Since we see a large class imbalance, we can run SMOTE to oversample the True class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1040,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2304, 3603)\n",
      "630\n",
      "(3348, 3603)\n",
      "1674\n"
     ]
    }
   ],
   "source": [
    "y = data['class']\n",
    "x = transformed_counts\n",
    "\n",
    "y_sparse = sparse_data['class']\n",
    "x_sparse = sparse_transformed_counts\n",
    "\n",
    "x_unbalanced, y_unbalanced = shuffle(x, y)\n",
    "x_sparse_unbalanced, y_sparse_unbalanced = shuffle(x_sparse, y_sparse)\n",
    "\n",
    "print(transformed_counts.shape)\n",
    "print(list(y_unbalanced).count(True))\n",
    "sm = SMOTE('minority')\n",
    "x_balanced, y_balanced = sm.fit_resample(transformed_counts, y)\n",
    "x_sparse_balanced, y_sparse_balanced = sm.fit_resample(sparse_transformed_counts, y_sparse)\n",
    "print(x_balanced.shape)\n",
    "print(list(y_balanced).count(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Model\n",
    "\n",
    "First we will try a Multinomial Naïve Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1041,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.5, 'fit_prior': False}\n",
      "0.7195988666532807\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "param_grid = {\n",
    "    'alpha': [i/10 for i in range(5,12,2)],\n",
    "    'fit_prior': (False, True),\n",
    "}\n",
    "# balanced data\n",
    "gridcv = GridSearchCV(mnb, param_grid)\n",
    "gridcv.fit(x_balanced, y_balanced)\n",
    "print(gridcv.best_params_)\n",
    "print(gridcv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's accuracy and recall scores are fair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1044,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision    recall  f1-score   support\n",
      "\n",
      "       False       0.78      0.66      0.71      1674\n",
      "        True       0.70      0.81      0.75      1674\n",
      "\n",
      "    accuracy                           0.74      3348\n",
      "   macro avg       0.74      0.74      0.73      3348\n",
      "weighted avg       0.74      0.74      0.73      3348\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# balanced data\n",
    "y_pred = cross_val_predict(gridcv, x_balanced, y_balanced, cv=10)\n",
    "report = classification_report(y_balanced, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the same model on unbalanced data gives us much worse results. Since the False class represents 73% of the data, the model vastly over predicts the 'False' label, giving it a fair overall accuracy but almost 0% recall of the True class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.5, 'fit_prior': True}\n",
      "0.726562293690465\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.73      1.00      0.84      1674\n",
      "        True       0.00      0.00      0.00       630\n",
      "\n",
      "    accuracy                           0.73      2304\n",
      "   macro avg       0.36      0.50      0.42      2304\n",
      "weighted avg       0.53      0.73      0.61      2304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unbalanced data\n",
    "gridcv.fit(x_unbalanced, y_unbalanced)\n",
    "print(gridcv.best_params_)\n",
    "print(gridcv.best_score_)\n",
    "y_pred = cross_val_predict(gridcv, x_unbalanced, y_unbalanced, cv=10)\n",
    "report = classification_report(y_unbalanced, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sparse data set gives us similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1046,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.7, 'fit_prior': False}\n",
      "0.613855386743413\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.78      0.66      0.71      1674\n",
      "        True       0.70      0.81      0.75      1674\n",
      "\n",
      "    accuracy                           0.74      3348\n",
      "   macro avg       0.74      0.74      0.73      3348\n",
      "weighted avg       0.74      0.74      0.73      3348\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sparse balanced data\n",
    "gridcv.fit(x_sparse_balanced, y_sparse_balanced)\n",
    "print(gridcv.best_params_)\n",
    "print(gridcv.best_score_)\n",
    "y_pred = cross_val_predict(gridcv, x_balanced, y_balanced, cv=10)\n",
    "report = classification_report(y_balanced, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1047,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.5, 'fit_prior': True}\n",
      "0.726562293690465\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.73      1.00      0.84      1674\n",
      "        True       0.00      0.00      0.00       630\n",
      "\n",
      "    accuracy                           0.73      2304\n",
      "   macro avg       0.36      0.50      0.42      2304\n",
      "weighted avg       0.53      0.73      0.61      2304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sparse unbalanced data\n",
    "gridcv = GridSearchCV(mnb, param_grid)\n",
    "gridcv.fit(x_sparse_unbalanced, y_sparse_unbalanced)\n",
    "print(gridcv.best_params_)\n",
    "print(gridcv.best_score_)\n",
    "y_pred = cross_val_predict(gridcv, x_sparse_unbalanced, y_sparse_unbalanced, cv=10)\n",
    "report = classification_report(y_sparse_unbalanced, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also trained a Complement Naïve Bayes model. Scikitlearn documentation indicates the Complement Naïve Bayes models tend to outperform Multinomial models in text classification tasks, and in particular are better at handling unbalanced data.\n",
    "However, it performed about on par with the Multinomial Naïve Bayes with balanced and unbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1048,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.5, 'fit_prior': False, 'norm': False}\n",
      "0.7195988666532807\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.78      0.66      0.71      1674\n",
      "        True       0.70      0.81      0.75      1674\n",
      "\n",
      "    accuracy                           0.74      3348\n",
      "   macro avg       0.74      0.74      0.73      3348\n",
      "weighted avg       0.74      0.74      0.73      3348\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnb = ComplementNB()\n",
    "param_grid = {\n",
    "    'alpha': [i/10 for i in range(5,12,2)],\n",
    "    'fit_prior': (False, True),\n",
    "    'norm': (False, True)\n",
    "}\n",
    "gridcv = GridSearchCV(cnb, param_grid)\n",
    "# with balanced data\n",
    "gridcv.fit(x_balanced, y_balanced)\n",
    "print(gridcv.best_params_)\n",
    "print(gridcv.best_score_)\n",
    "y_pred = cross_val_predict(gridcv, x_balanced, y_balanced, cv=10)\n",
    "report = classification_report(y_balanced, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.9, 'fit_prior': False, 'norm': True}\n",
      "0.7261256248231632\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.73      0.99      0.84      1674\n",
      "        True       0.25      0.00      0.01       630\n",
      "\n",
      "    accuracy                           0.72      2304\n",
      "   macro avg       0.49      0.50      0.42      2304\n",
      "weighted avg       0.60      0.72      0.61      2304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with unbalanced data\n",
    "gridcv.fit(x_unbalanced, y_unbalanced)\n",
    "print(gridcv.best_params_)\n",
    "print(gridcv.best_score_)\n",
    "y_pred = cross_val_predict(gridcv, x_unbalanced, y_unbalanced, cv=10)\n",
    "report = classification_report(y_unbalanced, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, though the CNB's performance on sparse balanced data is a little worse than that of MNB, it has a non-zero recall of 'False' on sparse *unbalanced* data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1050,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.7, 'fit_prior': False, 'norm': False}\n",
      "0.613855386743413\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.63      0.57      0.60      1674\n",
      "        True       0.61      0.66      0.63      1674\n",
      "\n",
      "    accuracy                           0.62      3348\n",
      "   macro avg       0.62      0.62      0.62      3348\n",
      "weighted avg       0.62      0.62      0.62      3348\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sparse balanced data\n",
    "gridcv.fit(x_sparse_balanced, y_sparse_balanced)\n",
    "print(gridcv.best_params_)\n",
    "print(gridcv.best_score_)\n",
    "y_pred = cross_val_predict(gridcv, x_sparse_balanced, y_sparse_balanced, cv=10)\n",
    "report = classification_report(y_sparse_balanced, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1051,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1.1, 'fit_prior': False, 'norm': True}\n",
      "0.6436659436008677\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.75      0.74      0.74      1674\n",
      "        True       0.33      0.35      0.34       630\n",
      "\n",
      "    accuracy                           0.63      2304\n",
      "   macro avg       0.54      0.54      0.54      2304\n",
      "weighted avg       0.64      0.63      0.63      2304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sparse unbalanced data\n",
    "gridcv.fit(x_sparse_unbalanced, y_sparse_unbalanced)\n",
    "print(gridcv.best_params_)\n",
    "print(gridcv.best_score_)\n",
    "y_pred = cross_val_predict(gridcv, x_sparse_unbalanced, y_sparse_unbalanced, cv=10)\n",
    "report = classification_report(y_sparse_unbalanced, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the only model that does more than predict 'False' overwhelmingly when trained on unbalanced data.\n",
    "\n",
    "Overall, our scores could have likely been improved with more data. Text classification is typically very data intensive, and we had to throw out the vast majority of unique words in order to achieve any results."
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('myvenv': venv)",
   "language": "python",
   "name": "python37464bitmyvenvvenv98d899ab2a1e48d996d97fcea0a28dce"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
